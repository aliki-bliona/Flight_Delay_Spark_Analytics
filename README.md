# âœˆï¸ Flight Delay Analytics with Apache Spark

This project focuses on Big Data processing and analysis using **Apache Spark**. We analyzed a large dataset of flight records to identify delay patterns, calculating departure delays and cancellation rates.

The core objective was to perform a **comparative analysis between Spark RDD and DataFrame APIs**, evaluating their execution efficiency and performance optimization mechanisms (Catalyst Optimizer).

## ğŸ“„ Project Report
For a detailed analysis of the methodology, performance metrics, and conclusions, please refer to the full report:
ğŸ‘‰ **[View Project Report (PDF)](Project_Report.pdf)**

## ğŸš€ Key Features

* **Big Data Processing:** Handling large-scale flight data using PySpark.
* **API Comparison:** Implementing the same queries using both **RDD** (Resilient Distributed Dataset) and **DataFrame** APIs to benchmark performance.
* **Data Cleaning:** Preprocessing raw CSV data, handling null values, and type casting.
* **Visualization:** Creating insight charts (Hourly Delays, Airline Delays) using `matplotlib` and `seaborn`.

## ğŸ“Š Performance Insights

As detailed in the report, the **DataFrame API** consistently outperformed RDDs due to:
* **Catalyst Optimizer:** Automatic query optimization plan.
* **Tungsten Execution Engine:** Optimized memory management and code generation.

## ğŸ› ï¸ Technologies Used

* **Language:** Python 3
* **Framework:** Apache Spark (PySpark)
* **Environment:** Google Colab
* **Libraries:** `pyspark`, `pandas`, `matplotlib`, `seaborn`

## âš™ï¸ How to Run

1.  Open the notebook `flight_delay_spark.ipynb` in **Google Colab** or a local Jupyter environment.
2.  Install the dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3.  Load your dataset (ensure the path in the code matches your file location).
4.  Run all cells to execute the Spark jobs.

## ğŸ‘¥ Authors

* **Aliki Bliona** - *Data Engineering & RDD Implementation*
* **Theodoros Panagiotidis** - *Data Analysis & Visualization*

---
*This project was developed as part of the "Big Data Analytics" course at the University of Macedonia.*